{"cells":[{"cell_type":"code","source":["\"\"\"\n","v8 (no_log): v7 ë¡œì§ + v5ì˜ ì´ì¤‘ í•„í„° ì ìš© (ë¡œê·¸ ë³€í™˜ ì œê±°)\n","- v7 (ê¸€ë¡œë²Œ RF ëª¨ë¸, v3 íŠ¹ì„± ê³µí•™)ì„ ê¸°ë°˜\n","- v5ì˜ ì´ì¤‘ í•„í„°(CCF + Pearson) ë¡œì§ì„ Step 2ì— ê²°í•©\n","- ì„ê³„ê°’: CCF > 0.30 AND Lagged Pearson > 0.30\n","- ëª¨ë¸: ë‹¨ì¼ ê¸€ë¡œë²Œ RandomForest\n","- [Fix]: .itertuples()ë¥¼ .iterrows()ë¡œ ë³€ê²½í•˜ì—¬ AttributeError í•´ê²°\n","- [Removed]: Target(y)ì— log1p ë³€í™˜ ì œê±° (ì›ë³¸ ê°’ìœ¼ë¡œ ì§ì ‘ ì˜ˆì¸¡)\n","- [New]: ì œì¶œ íŒŒì¼ì„ 9900ê°œê°€ ì•„ë‹Œ, í•„í„°ëœ ìŒìœ¼ë¡œë§Œ ìƒì„±\n","- [Fix]: 5ë‹¨ê³„(predict_v3)ì˜ NameError (if t >= 2) ìˆ˜ì •\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.ensemble import RandomForestRegressor\n","# [ì¶”ê°€] CCF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n","from statsmodels.tsa.stattools import ccf\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"=\" * 80)\n","print(\"ğŸš€ v8 (no_log): v7 ë¡œì§ + ì´ì¤‘ í•„í„°(CCF/Pearson) + RF (ë¡œê·¸ ë³€í™˜ ì—†ìŒ)\")\n","print(\"=\" * 80)\n","\n","# ============================================================================\n","# 1. ë°ì´í„° ë¡œë“œ ë° í”¼ë²— í…Œì´ë¸” ìƒì„±\n","# ============================================================================\n","print(\"\\n[1ë‹¨ê³„] ë°ì´í„° ë¡œë“œ ì¤‘...\")\n","try:\n","    train = pd.read_csv(\"train.csv\")\n","    submission_df = pd.read_csv(\"sample_submission.csv\")\n","except FileNotFoundError:\n","    print(\"ì˜¤ë¥˜: train.csv ë˜ëŠ” sample_submission.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","    exit()\n","\n","train['ym'] = pd.to_datetime(train['year'].astype(str) + '-' + train['month'].astype(str) + '-01')\n","\n","# Pivot í…Œì´ë¸” ìƒì„± (v3 Lagged Pearson ìš©)\n","pivot = train.groupby(['item_id', 'ym'])['value'].sum().unstack(fill_value=0)\n","# [ì¶”ê°€] ì°¨ë¶„(Diff) Pivot í…Œì´ë¸” ìƒì„± (v5 CCF ìš©)\n","pivot_diff = pivot.diff(axis=1).fillna(0)\n","\n","print(f\"Pivot shape: {pivot.shape}\")\n","\n","# ============================================================================\n","# 2. ê³µí–‰ì„± ìŒ íƒìƒ‰ (v5 + v7 ì´ì¤‘ í•„í„° ë¡œì§ ì ìš©)\n","# ============================================================================\n","print(\"\\n[2ë‹¨ê³„] ê³µí–‰ì„± ìŒ íƒìƒ‰ ì¤‘ (ì´ì¤‘ í•„í„°, CCF 0.30 + Pearson 0.30)...\")\n","\n","# v3/v7ì˜ Lagged Pearson í•¨ìˆ˜\n","def find_best_lag_corr_pearson(a_series_val, b_series_val, max_lag=12):\n","    best_corr = -1\n","    best_lag = 0\n","    for lag in range(max_lag + 1): # 0~12\n","        if lag >= len(a_series_val): break\n","        a_lagged = a_series_val[:-lag-1] if lag > 0 else a_series_val[:-1]\n","        b_target = b_series_val[lag+1:]\n","        if len(a_lagged) != len(b_target) or len(a_lagged) < 2: continue\n","        corr = np.corrcoef(a_lagged, b_target)[0, 1]\n","        if not np.isnan(corr) and corr > best_corr:\n","            best_corr = corr\n","            best_lag = lag\n","    return best_corr, best_lag\n","\n","# v5ì˜ CCF í•¨ìˆ˜ (ìµœëŒ€ê°’ ì°¾ê¸°)\n","def find_max_corr_ccf(a_series_diff_val, b_series_diff_val, max_lag=12):\n","    # CCFëŠ” lag 0 (ë™ì‹œ)ë¥¼ ì œì™¸í•˜ê³  1~max_lagì—ì„œ ì°¾ëŠ” ê²ƒì´ ì¼ë°˜ì \n","    if len(a_series_diff_val) < max_lag + 2 or len(b_series_diff_val) < max_lag + 2:\n","        return 0.0 # ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ 0\n","\n","    correlations = ccf(a_series_diff_val, b_series_diff_val, adjusted=False, nlags=max_lag)\n","    # lag 0 (ë™ì‹œ) ì œì™¸, 1~max_lag ì¤‘ ìµœëŒ€ê°’\n","    max_corr_ccf = np.max(np.abs(correlations[1:max_lag+1]))\n","    if np.isnan(max_corr_ccf):\n","        return 0.0\n","    return max_corr_ccf\n","\n","# submission_dfì˜ ëª¨ë“  ìŒì— ëŒ€í•´ *ë‘ ê°œì˜* ìƒê´€ê³„ìˆ˜ë¥¼ ëª¨ë‘ ê³„ì‚°\n","pairs = []\n","# [ìˆ˜ì •] itertuples() ëŒ€ì‹  iterrows()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì„± í™•ë³´\n","for index, row in tqdm(submission_df.iterrows(), total=len(submission_df), desc=\"ìŒ ê´€ê³„ íƒìƒ‰ (ì´ì¤‘ í•„í„°)\"):\n","    # [ìˆ˜ì •] . (ì ) ì ‘ê·¼ ëŒ€ì‹  [] (í‚¤) ì ‘ê·¼ìœ¼ë¡œ ë³€ê²½\n","    leader = row['leading_item_id']\n","    follower = row['following_item_id']\n","\n","    if leader in pivot.index and follower in pivot.index:\n","        # 1. Lagged Pearson ê³„ì‚° (v7 ë°©ì‹)\n","        a_series_orig = pivot.loc[leader].values\n","        b_series_orig = pivot.loc[follower].values\n","        max_corr_pearson, best_lag = find_best_lag_corr_pearson(a_series_orig, b_series_orig, max_lag=12)\n","\n","        # 2. CCF ê³„ì‚° (v5 ë°©ì‹)\n","        a_series_diff = pivot_diff.loc[leader].values\n","        b_series_diff = pivot_diff.loc[follower].values\n","        max_corr_ccf = find_max_corr_ccf(a_series_diff, b_series_diff, max_lag=12)\n","\n","        pairs.append({\n","            'leading_item_id': leader,\n","            'following_item_id': follower,\n","            'max_corr_pearson': max_corr_pearson,\n","            'max_corr_ccf': max_corr_ccf,\n","            'best_lag': best_lag # v7ì˜ pearson lagë¥¼ ë”°ë¦„\n","        })\n","\n","pairs_df = pd.DataFrame(pairs)\n","\n","# [ìˆ˜ì •] ì´ì¤‘ í•„í„° ì„ê³„ê°’\n","CORR_THRESHOLD_PEARSON = 0.30\n","CORR_THRESHOLD_CCF = 0.30\n","\n","filtered_pairs = pairs_df[\n","    (pairs_df['max_corr_pearson'] >= CORR_THRESHOLD_PEARSON) &\n","    (pairs_df['max_corr_ccf'] >= CORR_THRESHOLD_CCF)\n","].copy()\n","\n","# 'max_corr' ì»¬ëŸ¼ì„ v3 ë¡œì§(Pearson) ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ìƒì„± (í•™ìŠµ íŠ¹ì„±ìœ¼ë¡œ ì‚¬ìš©)\n","filtered_pairs['max_corr'] = filtered_pairs['max_corr_pearson']\n","\n","print(f\"\\nâœ… íƒìƒ‰ëœ ê³µí–‰ì„± ìŒ ìˆ˜ (ì´ì¤‘ í•„í„° í†µê³¼): {len(filtered_pairs)}\")\n","if len(filtered_pairs) > 0:\n","    print(f\"Pearson ìƒê´€ê³„ìˆ˜ ë²”ìœ„: {filtered_pairs['max_corr_pearson'].min():.3f} ~ {filtered_pairs['max_corr_pearson'].max():.3f}\")\n","    print(f\"CCF ìƒê´€ê³„ìˆ˜ ë²”ìœ„: {filtered_pairs['max_corr_ccf'].min():.3f} ~ {filtered_pairs['max_corr_ccf'].max():.3f}\")\n","else:\n","    print(\"ê²½ê³ : ì´ì¤‘ í•„í„°ë¥¼ í†µê³¼í•˜ëŠ” ìŒì´ ì—†ìŠµë‹ˆë‹¤. ì„ê³„ê°’ì„ ë‚®ì¶°ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n","\n","\n","# ============================================================================\n","# 3. Feature Engineering (Log ë³€í™˜ ì œê±°)\n","# ============================================================================\n","print(\"\\n[3ë‹¨ê³„] Feature Engineering ì¤‘...\")\n","\n","def create_features_v3(pivot_table, pairs_to_train):\n","    \"\"\"\n","    v3ì™€ ë™ì¼í•œ ê³ ê¸‰ Feature ìƒì„± í•¨ìˆ˜ (ë¡œê·¸ ë³€í™˜ ì—†ìŒ)\n","    \"\"\"\n","    months = pivot_table.columns.to_list()\n","    n_months = len(months)\n","    train_data = []\n","\n","    # [ìˆ˜ì •] .itertuples() -> .iterrows()\n","    for index, row in tqdm(pairs_to_train.iterrows(), total=len(pairs_to_train), desc=\"Feature ìƒì„±\"):\n","        # [ìˆ˜ì •] . -> []\n","        leader = row['leading_item_id']\n","        follower = row['following_item_id']\n","        lag = int(row['best_lag'])\n","        corr = float(row['max_corr']) # Lagged Pearson ê°’\n","\n","        a_series = pivot_table.loc[leader].values.astype(float)\n","        b_series = pivot_table.loc[follower].values.astype(float)\n","\n","        for t in range(lag + 6, n_months - 1): # ì¶©ë¶„í•œ ê³¼ê±° ë°ì´í„° í™•ë³´\n","            # ê¸°ë³¸ features\n","            b_t = b_series[t]\n","            b_t_1 = b_series[t - 1]\n","            a_t_lag = a_series[t - lag]\n","\n","            # [ìˆ˜ì •] Targetì— np.log1p(ë¡œê·¸ ë³€í™˜) ì œê±°\n","            target = b_series[t + 1] # ì›ë³¸ ê°’\n","\n","            # Lag features\n","            a_lag1 = a_series[t - 1] if t >= 1 else 0\n","            a_lag2 = a_series[t - 2] if t >= 2 else 0\n","            a_lag3 = a_series[t - 3] if t >= 3 else 0\n","            b_lag2 = b_series[t - 2] if t >= 2 else 0\n","            b_lag3 = b_series[t - 3] if t >= 3 else 0\n","\n","            # ì´ë™í‰ê·  (3ê°œì›”, 6ê°œì›”)\n","            b_ma3 = np.mean(b_series[max(0, t-2):t+1]) if t >= 2 else b_t\n","            b_ma6 = np.mean(b_series[max(0, t-5):t+1]) if t >= 5 else b_t\n","            a_ma3 = np.mean(a_series[max(0, t-2):t+1]) if t >= 2 else a_t_lag\n","\n","            # ë³€í™”ìœ¨ (MoM)\n","            b_mom = (b_t - b_t_1) / (b_t_1 + 1) if b_t_1 > 0 else 0\n","            a_mom = (a_t_lag - a_lag1) / (a_lag1 + 1) if a_lag1 > 0 else 0\n","\n","            # ê³„ì ˆì„± (ì›”)\n","            month = pd.to_datetime(months[t]).month\n","            is_jan = 1 if month == 1 else 0\n","            is_sep = 1 if month == 9 else 0\n","\n","            # íŠ¸ë Œë“œ\n","            year = pd.to_datetime(months[t]).year\n","            year_effect = year - 2022\n","\n","            train_data.append({\n","                'b_t': b_t, 'b_t_1': b_t_1, 'a_t_lag': a_t_lag,\n","                'max_corr': corr, 'best_lag': float(lag),\n","                'a_lag1': a_lag1, 'a_lag2': a_lag2, 'a_lag3': a_lag3,\n","                'b_lag2': b_lag2, 'b_lag3': b_lag3,\n","                'b_ma3': b_ma3, 'b_ma6': b_ma6, 'a_ma3': a_ma3,\n","                'b_mom': b_mom, 'a_mom': a_mom,\n","                'is_jan': is_jan, 'is_sep': is_sep, 'month': month,\n","                'year_effect': year_effect,\n","                'target': target # ì›ë³¸ íƒ€ê²Ÿ ì €ì¥\n","            })\n","\n","    return pd.DataFrame(train_data)\n","\n","if len(filtered_pairs) > 0:\n","    df_train = create_features_v3(pivot, filtered_pairs)\n","    print(f\"\\ní•™ìŠµ ë°ì´í„° shape: {df_train.shape}\")\n","    if df_train.empty:\n","        print(\"ê²½ê³ : í•„í„°ëœ ìŒì´ ìˆìœ¼ë‚˜ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë°ì´í„° ê¸°ê°„ ë¶€ì¡± ë“±)\")\n","        feature_cols = []\n","    else:\n","        print(f\"Feature ê°œìˆ˜: {len(df_train.columns) - 1}\")\n","        feature_cols = [col for col in df_train.columns if col != 'target']\n","else:\n","    df_train = pd.DataFrame()\n","    feature_cols = []\n","\n","# ============================================================================\n","# 4. RandomForest ëª¨ë¸ í•™ìŠµ (v7ê³¼ ë™ì¼)\n","# ============================================================================\n","print(\"\\n[4ë‹¨ê³„] RandomForest ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n","\n","if not df_train.empty and feature_cols:\n","    train_X = df_train[feature_cols].values\n","    train_y = df_train['target'].values # ì›ë³¸ íƒ€ê²Ÿìœ¼ë¡œ í•™ìŠµ\n","\n","    model = RandomForestRegressor(\n","        n_estimators=200,\n","        max_depth=6,\n","        min_samples_leaf=20,\n","        random_state=42,\n","        n_jobs=-1\n","    )\n","\n","    model.fit(train_X, train_y)\n","    print(\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n","\n","    # Feature Importance\n","    feature_importance = pd.DataFrame({\n","        'feature': feature_cols,\n","        'importance': model.feature_importances_\n","    }).sort_values('importance', ascending=False)\n","\n","    print(f\"\\nTop 10 ì¤‘ìš” Features:\")\n","    print(feature_importance.head(10).to_string())\n","else:\n","    print(\"í•™ìŠµ ë°ì´í„°ê°€ ì—†ì–´ ëª¨ë¸ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n","    model = None # ëª¨ë¸ì´ ì—†ìŒì„ ëª…ì‹œ\n","\n","# ============================================================================\n","# 5. ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± (í•„í„°ëœ 2256ê°œë§Œ)\n","# ============================================================================\n","print(\"\\n[5ë‹¨ê³„] ì˜ˆì¸¡ ì¤‘...\")\n","\n","def predict_v3(pivot_table, pairs_to_predict, model_to_use, feature_cols_list):\n","    \"\"\"\n","    v3ì™€ ë™ì¼í•œ ì˜ˆì¸¡ í•¨ìˆ˜ (ë¡œê·¸ ë³€í™˜ ì—†ìŒ)\n","    \"\"\"\n","    months = pivot_table.columns.to_list()\n","    n_months = len(months)\n","    t_last = n_months - 1\n","    t_prev = n_months - 2\n","\n","    preds = {} # ë”•ì…”ë„ˆë¦¬ë¡œ ì˜ˆì¸¡ê°’ ì €ì¥ (ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•´)\n","\n","    # [ìˆ˜ì •] .itertuples() -> .iterrows()\n","    for index, row in tqdm(pairs_to_predict.iterrows(), total=len(pairs_to_predict), desc=\"ì˜ˆì¸¡ ì¤‘\"):\n","        # [ìˆ˜ì •] . -> []\n","        leader = row['leading_item_id']\n","        follower = row['following_item_id']\n","        lag = int(row['best_lag'])\n","        corr = float(row['max_corr']) # Lagged Pearson ê°’\n","\n","        if leader not in pivot_table.index or follower not in pivot_table.index:\n","            continue\n","\n","        a_series = pivot_table.loc[leader].values.astype(float)\n","        b_series = pivot_table.loc[follower].values.astype(float)\n","\n","        if t_last - lag < 0:\n","            continue\n","\n","        # v3ì™€ ë™ì¼í•œ ì˜ˆì¸¡ìš© íŠ¹ì„± ìƒì„±\n","        b_t = b_series[t_last]\n","        b_t_1 = b_series[t_prev]\n","        a_t_lag = a_series[t_last - lag]\n","\n","        a_lag1 = a_series[t_last - 1]\n","\n","        # [ìˆ˜ì •] NameError: 't' -> 't_last'\n","        a_lag2 = a_series[t_last - 2] if t_last >= 2 else 0\n","        a_lag3 = a_series[t_last - 3] if t_last >= 3 else 0\n","        b_lag2 = b_series[t_last - 2] if t_last >= 2 else 0\n","        b_lag3 = b_series[t_last - 3] if t_last >= 3 else 0\n","\n","        b_ma3 = np.mean(b_series[max(0, t_last-2):t_last+1])\n","        b_ma6 = np.mean(b_series[max(0, t_last-5):t_last+1])\n","        a_ma3 = np.mean(a_series[max(0, t_last-2):t_last+1])\n","\n","        b_mom = (b_t - b_t_1) / (b_t_1 + 1) if b_t_1 > 0 else 0\n","        a_mom = (a_t_lag - a_lag1) / (a_lag1 + 1) if a_lag1 > 0 else 0\n","\n","        month = 8 # 2025ë…„ 8ì›” ì˜ˆì¸¡\n","        is_jan = 0\n","        is_sep = 0\n","        year_effect = 2025 - 2022\n","\n","        features = {\n","            'b_t': b_t, 'b_t_1': b_t_1, 'a_t_lag': a_t_lag,\n","            'max_corr': corr, 'best_lag': float(lag),\n","            'a_lag1': a_lag1, 'a_lag2': a_lag2, 'a_lag3': a_lag3,\n","            'b_lag2': b_lag2, 'b_lag3': b_lag3,\n","            'b_ma3': b_ma3, 'b_ma6': b_ma6, 'a_ma3': a_ma3,\n","            'b_mom': b_mom, 'a_mom': a_mom,\n","            'is_jan': is_jan, 'is_sep': is_sep, 'month': month,\n","            'year_effect': year_effect\n","        }\n","\n","        X_test = np.array([[features[col] for col in feature_cols_list]])\n","\n","        # [ìˆ˜ì •] ëª¨ë¸ì€ ì›ë³¸ ê°’ì„ ì˜ˆì¸¡ (y_pred)\n","        y_pred = model_to_use.predict(X_test)[0]\n","\n","        # [ìˆ˜ì •] np.expm1(ì§€ìˆ˜ ë³€í™˜) ì œê±°\n","\n","        y_pred = max(0.0, float(y_pred))\n","        y_pred = int(round(y_pred))\n","\n","        # (leader, follower) íŠœí”Œì„ í‚¤ë¡œ ì‚¬ìš©\n","        preds[(leader, follower)] = y_pred\n","\n","    return preds\n","\n","# í•™ìŠµëœ ëª¨ë¸ê³¼ í•„í„°ëœ ìŒìœ¼ë¡œ ì˜ˆì¸¡ ì‹¤í–‰\n","if model and not filtered_pairs.empty:\n","    predictions_dict = predict_v3(pivot, filtered_pairs, model, feature_cols)\n","    print(f\"\\nâœ… ì˜ˆì¸¡ ì™„ë£Œ! {len(predictions_dict)}ê°œì˜ ìŒì„ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.\")\n","else:\n","    print(\"í•™ìŠµëœ ëª¨ë¸ì´ ì—†ê±°ë‚˜ í•„í„°ëœ ìŒì´ ì—†ì–´ ì˜ˆì¸¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n","    predictions_dict = {}\n","\n","# --- [ìˆ˜ì •] ì œì¶œ íŒŒì¼ ìƒì„± (í•„í„°ëœ ìŒë§Œ) ---\n","# 1. ë¶„ì„ìš© íŒŒì¼(filtered_pairs_df) ì €ì¥ì„ ìœ„í•´ ë”•ì…”ë„ˆë¦¬ë¥¼ ë³‘í•©\n","filtered_pairs_with_preds = filtered_pairs.copy()\n","filtered_pairs_with_preds['value'] = filtered_pairs_with_preds.apply(\n","    lambda row: predictions_dict.get((row['leading_item_id'], row['following_item_id']), 0),\n","    axis=1\n",")\n","\n","# 2. ìš”ì²­í•˜ì‹  'ì œì¶œìš©' íŒŒì¼ ì €ì¥ (í•„í„°ëœ 2256ê°œë§Œ)\n","# 'value' ì»¬ëŸ¼ì„ í¬í•¨í•˜ì—¬ sample_submission í˜•ì‹ì— ë§ì¶¤\n","output_path = 'submission_v8_filtered_only.csv'\n","final_submission_df = filtered_pairs_with_preds[[\n","    'leading_item_id', 'following_item_id', 'value'\n","]]\n","final_submission_df.to_csv(output_path, index=False)\n","\n","print(f\"\\nâœ… [ì œì¶œìš©] í•„í„°ëœ ìŒ íŒŒì¼ ì €ì¥: {output_path}\")\n","\n","# 3. (ì œê±°) 9900ê°œì§œë¦¬ íŒŒì¼ ìƒì„± ë¡œì§ ì œê±°\n","\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"v8 (RF + ì´ì¤‘ í•„í„° + Logë³€í™˜ ì—†ìŒ / í•„í„°ëœ ìŒë§Œ ì œì¶œ) ìƒì„± ì™„ë£Œ! ğŸ‰\") # [ìˆ˜ì •]\n","print(\"=\" * 80)\n","print(f\"ì´ {len(filtered_pairs)}ê°œì˜ ìŒì„ ì˜ˆì¸¡í•˜ì—¬ íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n","print(f\"ì˜ˆì¸¡ê°’ í†µê³„ (0 ì œì™¸):\")\n","# 0ìœ¼ë¡œë§Œ ì˜ˆì¸¡ëœ ê²½ìš° describe() ì˜¤ë¥˜ ë°©ì§€\n","if len(predictions_dict) > 0 and final_submission_df[final_submission_df['value'] > 0].empty == False:\n","    print(final_submission_df[final_submission_df['value'] > 0]['value'].describe())\n","else:\n","    print(\"0ë³´ë‹¤ í° ì˜ˆì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤.\")\n","print(\"=\" * 80)"],"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","ğŸš€ v8 (no_log): v7 ë¡œì§ + ì´ì¤‘ í•„í„°(CCF/Pearson) + RF (ë¡œê·¸ ë³€í™˜ ì—†ìŒ)\n","================================================================================\n","\n","[1ë‹¨ê³„] ë°ì´í„° ë¡œë“œ ì¤‘...\n","Pivot shape: (100, 43)\n","\n","[2ë‹¨ê³„] ê³µí–‰ì„± ìŒ íƒìƒ‰ ì¤‘ (ì´ì¤‘ í•„í„°, CCF 0.30 + Pearson 0.30)...\n"]},{"output_type":"stream","name":"stderr","text":["ìŒ ê´€ê³„ íƒìƒ‰ (ì´ì¤‘ í•„í„°): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9900/9900 [00:25<00:00, 381.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… íƒìƒ‰ëœ ê³µí–‰ì„± ìŒ ìˆ˜ (ì´ì¤‘ í•„í„° í†µê³¼): 2256\n","Pearson ìƒê´€ê³„ìˆ˜ ë²”ìœ„: 0.300 ~ 0.929\n","CCF ìƒê´€ê³„ìˆ˜ ë²”ìœ„: 0.300 ~ 0.875\n","\n","[3ë‹¨ê³„] Feature Engineering ì¤‘...\n"]},{"output_type":"stream","name":"stderr","text":["Feature ìƒì„±: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [00:02<00:00, 1105.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","í•™ìŠµ ë°ì´í„° shape: (67349, 20)\n","Feature ê°œìˆ˜: 19\n","\n","[4ë‹¨ê³„] RandomForest ëª¨ë¸ í•™ìŠµ ì¤‘...\n","âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n","\n","Top 10 ì¤‘ìš” Features:\n","        feature  importance\n","11        b_ma6    0.623829\n","10        b_ma3    0.327448\n","1         b_t_1    0.021410\n","9        b_lag3    0.008059\n","0           b_t    0.007571\n","13        b_mom    0.003997\n","8        b_lag2    0.003173\n","17        month    0.002051\n","18  year_effect    0.001702\n","15       is_jan    0.000549\n","\n","[5ë‹¨ê³„] ì˜ˆì¸¡ ì¤‘...\n"]},{"output_type":"stream","name":"stderr","text":["ì˜ˆì¸¡ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [02:30<00:00, 15.01it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… ì˜ˆì¸¡ ì™„ë£Œ! 2256ê°œì˜ ìŒì„ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.\n","\n","âœ… [ì œì¶œìš©] í•„í„°ëœ ìŒ íŒŒì¼ ì €ì¥: submission_v8_filtered_only.csv\n","\n","================================================================================\n","v8 (RF + ì´ì¤‘ í•„í„° + Logë³€í™˜ ì—†ìŒ / í•„í„°ëœ ìŒë§Œ ì œì¶œ) ìƒì„± ì™„ë£Œ! ğŸ‰\n","================================================================================\n","ì´ 2256ê°œì˜ ìŒì„ ì˜ˆì¸¡í•˜ì—¬ íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n","ì˜ˆì¸¡ê°’ í†µê³„ (0 ì œì™¸):\n","count    2.256000e+03\n","mean     3.218406e+06\n","std      1.029510e+07\n","min      1.993900e+04\n","25%      1.993900e+04\n","50%      3.448830e+05\n","75%      2.509248e+06\n","max      1.026703e+08\n","Name: value, dtype: float64\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_aRcO2PlPzvi","executionInfo":{"status":"ok","timestamp":1763391112182,"user_tz":-540,"elapsed":235723,"user":{"displayName":"ìƒí˜","userId":"11228226743755553046"}},"outputId":"11bc69f4-69d2-4f30-cecd-d71bc95ae68e"}},{"cell_type":"code","source":[],"metadata":{"id":"a7EqsAZxP4ss"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}