{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb+x7G3DRTlof0v+ZmoTfd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHY3066/MiChung/blob/main/%EC%A0%9C%EC%B6%9C%ED%8C%8C%EC%9D%BC/gmu_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"ğŸš€ 0.4ì  ëŒíŒŒë¥¼ ìœ„í•œ ìµœì¢… í•©ì²´ ì‹œì‘...\")\n",
        "\n",
        "# 1. ë°ì´í„° ë¡œë“œ\n",
        "# (íŒŒì¼ëª…ì´ ì •í™•í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”)\n",
        "file_base = 'common_pairs_best_values_1620.csv'  # 0.311ì  ê¸°ë¡í•œ íŒŒì¼ (ìš°ë¦¬ì˜ Base)\n",
        "file_source = 'v3_lightgbm_submission.csv'     # ê°’ì„ ê°€ì ¸ì˜¬ 1ë“± íŒŒì¼\n",
        "train_file = 'train.csv'\n",
        "\n",
        "df_base = pd.read_csv(file_base)\n",
        "df_source = pd.read_csv(file_source)\n",
        "train = pd.read_csv(train_file)\n",
        "\n",
        "# Pair ì‹ë³„ì ìƒì„±\n",
        "df_base['pair'] = df_base['leading_item_id'] + '_' + df_base['following_item_id']\n",
        "df_source['pair'] = df_source['leading_item_id'] + '_' + df_source['following_item_id']\n",
        "\n",
        "# 2. í›„ë³´êµ° ì„ ì • (Baseì—ëŠ” ì—†ì§€ë§Œ 1ë“± íŒŒì¼ì—ëŠ” ìˆì—ˆë˜ 'ë²„ë ¤ì§„ ìŒ'ë“¤)\n",
        "base_pairs = set(df_base['pair'])\n",
        "source_pairs = set(df_source[df_source['value'] > 0]['pair'])\n",
        "candidates = list(source_pairs - base_pairs)\n",
        "\n",
        "print(f\"âœ… ê¸°ì¡´ ë² ì´ìŠ¤ ìŒ: {len(base_pairs)}ê°œ\")\n",
        "print(f\"ğŸ” ì •ë°€ ê²€ì¦í•  í›„ë³´ ìŒ: {len(candidates)}ê°œ\")\n",
        "\n",
        "# 3. ë°ì´í„° í”¼ë²— (ê²€ì¦ìš©)\n",
        "train['date'] = pd.to_datetime(train['year'].astype(str) + '-' + train['month'].astype(str) + '-01')\n",
        "pivot_df = train.pivot_table(index='date', columns='item_id', values='value', aggfunc='sum').fillna(0)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ì „ëµ 1: TLCC (Time Lagged Cross Correlation) - ì‹œì°¨ 1~6ê°œì›” ìŠ¤ìº”\n",
        "# ---------------------------------------------------------\n",
        "tlcc_rescued = []\n",
        "tlcc_threshold = 0.65 # ê¸°ì¤€ê°’\n",
        "\n",
        "for pair in candidates:\n",
        "    lead, follow = pair.split('_')\n",
        "    if lead in pivot_df.columns and follow in pivot_df.columns:\n",
        "        s_lead = pivot_df[lead]\n",
        "        s_follow = pivot_df[follow]\n",
        "\n",
        "        max_corr = 0\n",
        "        # 1ê°œì›”ë¶€í„° 6ê°œì›”ê¹Œì§€ ë°€ì–´ë³´ë©° í™•ì¸\n",
        "        for lag in range(1, 7):\n",
        "            if len(s_lead) > lag:\n",
        "                # Lead(t-lag) vs Follow(t)\n",
        "                corr = np.corrcoef(s_lead.iloc[:-lag], s_follow.iloc[lag:])[0, 1]\n",
        "                if corr > max_corr:\n",
        "                    max_corr = corr\n",
        "\n",
        "        if max_corr > tlcc_threshold:\n",
        "            tlcc_rescued.append(pair)\n",
        "\n",
        "print(f\"ğŸ’ TLCC(ì‹œì°¨ë¶„ì„)ë¡œ ë°œêµ´í•œ ìŒ: {len(tlcc_rescued)}ê°œ\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ì „ëµ 2: Granger Causality Test - ì¸ê³¼ê´€ê³„ ê²€ì¦\n",
        "# ---------------------------------------------------------\n",
        "granger_rescued = []\n",
        "# ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ TLCCì—ì„œ ëª» ì°¾ì€ ì• ë“¤ ìœ„ì£¼ë¡œ ë³´ê±°ë‚˜, ì „ì²´ë¥¼ ë‹¤ ë´ë„ ë¨ (ì—¬ê¸°ì„  ì „ì²´ ë´…ë‹ˆë‹¤)\n",
        "for pair in candidates:\n",
        "    lead, follow = pair.split('_')\n",
        "    if lead in pivot_df.columns and follow in pivot_df.columns:\n",
        "        data = pivot_df[[follow, lead]]\n",
        "        try:\n",
        "            # Lag 1~3ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸\n",
        "            gc_res = grangercausalitytests(data, maxlag=3, verbose=False)\n",
        "            p_values = [gc_res[lag][0]['ssr_ftest'][1] for lag in range(1, 4)]\n",
        "            min_p = min(p_values)\n",
        "\n",
        "            if min_p < 0.05: # ìœ ì˜ìˆ˜ì¤€ 0.05 ë¯¸ë§Œ (ì¸ê³¼ê´€ê³„ ìˆìŒ)\n",
        "                granger_rescued.append(pair)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(f\"ğŸ”¥ Granger(ì¸ê³¼ê´€ê³„)ë¡œ ë°œêµ´í•œ ìŒ: {len(granger_rescued)}ê°œ\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. ìµœì¢… í•©ì²´ (Base + TLCC + Granger)\n",
        "# ---------------------------------------------------------\n",
        "final_pairs = base_pairs.union(set(tlcc_rescued)).union(set(granger_rescued))\n",
        "\n",
        "# 1ë“± íŒŒì¼ì—ì„œ í•´ë‹¹ ìŒë“¤ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
        "final_df = df_source[df_source['pair'].isin(final_pairs)].copy()\n",
        "\n",
        "# ì €ì¥\n",
        "output_name = 'submission_final_ensemble_04_aiming.csv'\n",
        "final_df[['leading_item_id', 'following_item_id', 'value']].to_csv(output_name, index=False)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"ğŸ‰ ìµœì¢… íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_name}\")\n",
        "print(f\"ğŸ“Š ì´ ì œì¶œ ìŒ ê°œìˆ˜: {len(final_df)}ê°œ (ê¸°ì¡´ {len(base_pairs)} + ì¶”ê°€ {len(final_df) - len(base_pairs)})\")\n",
        "print(\"ì´ íŒŒì¼ë¡œ 0.4ì  ëŒíŒŒì— ë„ì „í•˜ì„¸ìš”!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRa3yWK2i27O",
        "outputId": "4f14ceeb-c342-4df8-b1b8-2e5b12f377e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ 0.4ì  ëŒíŒŒë¥¼ ìœ„í•œ ìµœì¢… í•©ì²´ ì‹œì‘...\n",
            "âœ… ê¸°ì¡´ ë² ì´ìŠ¤ ìŒ: 1620ê°œ\n",
            "ğŸ” ì •ë°€ ê²€ì¦í•  í›„ë³´ ìŒ: 538ê°œ\n",
            "ğŸ’ TLCC(ì‹œì°¨ë¶„ì„)ë¡œ ë°œêµ´í•œ ìŒ: 32ê°œ\n",
            "ğŸ”¥ Granger(ì¸ê³¼ê´€ê³„)ë¡œ ë°œêµ´í•œ ìŒ: 202ê°œ\n",
            "------------------------------\n",
            "ğŸ‰ ìµœì¢… íŒŒì¼ ìƒì„± ì™„ë£Œ: submission_final_ensemble_04_aiming.csv\n",
            "ğŸ“Š ì´ ì œì¶œ ìŒ ê°œìˆ˜: 1836ê°œ (ê¸°ì¡´ 1620 + ì¶”ê°€ 216)\n",
            "ì´ íŒŒì¼ë¡œ 0.4ì  ëŒíŒŒì— ë„ì „í•˜ì„¸ìš”!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (íŒŒì¼ëª…ì´ ì •í™•í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”)\n",
        "# (1) ìš°ë¦¬ê°€ ë§Œë“  1,836ê°œì§œë¦¬ íƒ€ê²Ÿ íŒŒì¼\n",
        "target_file = 'submission_final_ensemble_04_aiming.csv'\n",
        "\n",
        "# (2) ì•™ìƒë¸”ì— ì‚¬ìš©í•  3ê°œ ë² ì´ìŠ¤ íŒŒì¼\n",
        "file_lgbm = 'v3_lightgbm_submission.csv'\n",
        "file_pearson = 'submission_single_filter_pearson_filtered_only_0.35.csv'\n",
        "file_sub3 = 'submission 0.3.csv'\n",
        "\n",
        "df_target = pd.read_csv(target_file)\n",
        "df_lgbm = pd.read_csv(file_lgbm)\n",
        "df_pearson = pd.read_csv(file_pearson)\n",
        "df_sub3 = pd.read_csv(file_sub3)\n",
        "\n",
        "print(\"íŒŒì¼ ë¡œë“œ ì™„ë£Œ!\")\n",
        "\n",
        "# 2. Pair ì‹ë³„ì ìƒì„± (ì„ í–‰_í›„í–‰)\n",
        "for df in [df_target, df_lgbm, df_pearson, df_sub3]:\n",
        "    df['pair'] = df['leading_item_id'] + '_' + df['following_item_id']\n",
        "\n",
        "# 3. ê°’ ë§¤í•‘ì„ ìœ„í•œ ì¸ë±ìŠ¤ ì„¤ì • (ë¹ ë¥¸ ê²€ìƒ‰ìš©)\n",
        "val_lgbm = df_lgbm.set_index('pair')['value']\n",
        "val_pearson = df_pearson.set_index('pair')['value']\n",
        "val_sub3 = df_sub3.set_index('pair')['value']\n",
        "\n",
        "# 4. ê³µí†µ ìŒ(Common Pairs) ì‹ë³„\n",
        "# 3ê°œ íŒŒì¼ ëª¨ë‘ê°€ \"ê³µí–‰ì„± ìˆë‹¤(Value > 0)\"ê³  íŒë‹¨í•œ êµì§‘í•©\n",
        "pairs_lgbm = set(df_lgbm[df_lgbm['value'] > 0]['pair'])\n",
        "pairs_pearson = set(df_pearson[df_pearson['value'] > 0]['pair'])\n",
        "pairs_sub3 = set(df_sub3[df_sub3['value'] > 0]['pair'])\n",
        "\n",
        "common_pairs = pairs_lgbm.intersection(pairs_pearson).intersection(pairs_sub3)\n",
        "\n",
        "print(f\"âœ… 3ê°œ ëª¨ë¸ ê³µí†µ ìŒ: {len(common_pairs)}ê°œ (í‰ê· ê°’ ì ìš©)\")\n",
        "print(f\"â­ ìƒˆë¡œ ì¶”ê°€ëœ íˆë“  ìŒ: {len(df_target) - len(common_pairs)}ê°œ (1ë“± ëª¨ë¸ ê°’ ìœ ì§€)\")\n",
        "\n",
        "# 5. í•˜ì´ë¸Œë¦¬ë“œ ê°’ ê³„ì‚° & ì •ìˆ˜ ë³€í™˜\n",
        "new_values = []\n",
        "\n",
        "for idx, row in df_target.iterrows():\n",
        "    pair = row['pair']\n",
        "\n",
        "    if pair in common_pairs:\n",
        "        # [ì „ëµ A] ê³µí†µ ìŒì´ë©´ -> 3ê°œ ëª¨ë¸ì˜ í‰ê· ê°’ ì‚¬ìš© (NMAE ì˜¤ì°¨ ìµœì†Œí™”)\n",
        "        # .get(pair, 0)ì€ í˜¹ì‹œ ê°’ì´ ì—†ì„ ê²½ìš° ì—ëŸ¬ ë°©ì§€ìš©\n",
        "        mean_val = (val_lgbm.get(pair, 0) + val_pearson.get(pair, 0) + val_sub3.get(pair, 0)) / 3\n",
        "        new_values.append(mean_val)\n",
        "    else:\n",
        "        # [ì „ëµ B] ì¶”ê°€ëœ íˆë“  ìŒì´ë©´ -> ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ LightGBM ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
        "        # (ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ ì´ ìŒì„ 0ìœ¼ë¡œ ì˜ˆì¸¡í–ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ì•„ í‰ê·  ë‚´ë©´ ê°’ì´ ë§ê°€ì§)\n",
        "        new_values.append(row['value'])\n",
        "\n",
        "# ê°’ ì—…ë°ì´íŠ¸\n",
        "df_target['value'] = new_values\n",
        "\n",
        "# â˜… í•µì‹¬: ì˜ˆì¸¡ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜(int)ë¡œ ë³€í™˜ (ëŒ€íšŒ ê·œì¹™ ì¤€ìˆ˜)\n",
        "df_target['value'] = df_target['value'].round().astype(int)\n",
        "\n",
        "# 6. ìµœì¢… íŒŒì¼ ì €ì¥\n",
        "output_name = 'submission_final_ensemble_04_aiming_mean_int.csv'\n",
        "df_target[['leading_item_id', 'following_item_id', 'value']].to_csv(output_name, index=False)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"ğŸ‰ íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_name}\")\n",
        "print(f\"ë°ì´í„° í™•ì¸:\\n{df_target.head()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwmiyxWct-nn",
        "outputId": "8780f467-1c69-4254-efcd-3b002515dbf5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "íŒŒì¼ ë¡œë“œ ì™„ë£Œ!\n",
            "âœ… 3ê°œ ëª¨ë¸ ê³µí†µ ìŒ: 1620ê°œ (í‰ê· ê°’ ì ìš©)\n",
            "â­ ìƒˆë¡œ ì¶”ê°€ëœ íˆë“  ìŒ: 216ê°œ (1ë“± ëª¨ë¸ ê°’ ìœ ì§€)\n",
            "------------------------------\n",
            "ğŸ‰ íŒŒì¼ ìƒì„± ì™„ë£Œ: submission_final_ensemble_04_aiming_mean_int.csv\n",
            "ë°ì´í„° í™•ì¸:\n",
            "  leading_item_id following_item_id    value               pair\n",
            "0        AANGBULD          DDEXPPXU    16186  AANGBULD_DDEXPPXU\n",
            "1        AANGBULD          DEWLVASR   321588  AANGBULD_DEWLVASR\n",
            "2        AANGBULD          EVBVXETX  4971642  AANGBULD_EVBVXETX\n",
            "3        AANGBULD          FITUEHWN    75752  AANGBULD_FITUEHWN\n",
            "4        AANGBULD          FTSVTTSR   128126  AANGBULD_FTSVTTSR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (ê²½ë¡œ/íŒŒì¼ëª… í™•ì¸í•´ì£¼ì„¸ìš”)\n",
        "# (1) ìš°ë¦¬ê°€ ë§Œë“  0.311ì ì§œë¦¬ ë² ì´ìŠ¤ íŒŒì¼ (1,620ê°œ)\n",
        "file_common = 'common_pairs_ensemble_1620_clean.csv'\n",
        "# (2) í›„ë³´ë¥¼ ê°€ì ¸ì˜¬ 1ë“± íŒŒì¼ (LightGBM)\n",
        "file_lgbm = 'v3_lightgbm_submission.csv'\n",
        "# (3) ìƒê´€ê³„ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ í•™ìŠµ ë°ì´í„°\n",
        "file_train = 'train.csv'\n",
        "\n",
        "df_common = pd.read_csv(file_common)\n",
        "df_lgbm = pd.read_csv(file_lgbm)\n",
        "train = pd.read_csv(file_train)\n",
        "\n",
        "print(\"íŒŒì¼ ë¡œë“œ ì™„ë£Œ!\")\n",
        "\n",
        "# 2. Pair ì‹ë³„ì ìƒì„±\n",
        "df_common['pair'] = df_common['leading_item_id'] + '_' + df_common['following_item_id']\n",
        "df_lgbm['pair'] = df_lgbm['leading_item_id'] + '_' + df_lgbm['following_item_id']\n",
        "\n",
        "# 3. ì¶”ê°€í•  í›„ë³´êµ°(Lost Pairs) ì°¾ê¸°\n",
        "# LightGBMì—ëŠ” ìˆëŠ”ë°, ì•ˆì „ë¹µ íŒŒì¼(Common)ì—ëŠ” ì—†ëŠ” ê²ƒë“¤\n",
        "pairs_common = set(df_common['pair'])\n",
        "pairs_lgbm = set(df_lgbm[df_lgbm['value'] > 0]['pair'])\n",
        "\n",
        "lost_pairs = list(pairs_lgbm - pairs_common)\n",
        "print(f\"ë² ì´ìŠ¤ íŒŒì¼ ìŒ: {len(pairs_common)}ê°œ\")\n",
        "print(f\"ì¶”ê°€ ê²€ì¦í•  í›„ë³´ ìŒ: {len(lost_pairs)}ê°œ\")\n",
        "\n",
        "# 4. í›„ë³´êµ° ì ìˆ˜ ë§¤ê¸°ê¸° (ìƒê´€ê³„ìˆ˜ ë¶„ì„)\n",
        "# ë°ì´í„° í”¼ë²— (ë‚ ì§œë³„ ì •ë ¬)\n",
        "train['date'] = pd.to_datetime(train['year'].astype(str) + '-' + train['month'].astype(str) + '-01')\n",
        "pivot_df = train.pivot_table(index='date', columns='item_id', values='value', aggfunc='sum').fillna(0)\n",
        "\n",
        "scored_candidates = []\n",
        "\n",
        "for pair in lost_pairs:\n",
        "    lead, follow = pair.split('_')\n",
        "\n",
        "    # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ê³„ì‚°\n",
        "    if lead in pivot_df.columns and follow in pivot_df.columns:\n",
        "        s_lead = pivot_df[lead]\n",
        "        s_follow = pivot_df[follow]\n",
        "\n",
        "        # ìµœëŒ€ ìƒê´€ê³„ìˆ˜ ì°¾ê¸° (Lag 0 ~ 6ê°œì›”)\n",
        "        corrs = []\n",
        "        # (1) ë‹¨ìˆœ ìƒê´€ê³„ìˆ˜ (Lag 0)\n",
        "        corrs.append(s_lead.corr(s_follow))\n",
        "        # (2) ì‹œì°¨ ìƒê´€ê³„ìˆ˜ (Lag 1~6)\n",
        "        for lag in range(1, 7):\n",
        "            if len(s_lead) > lag:\n",
        "                c = np.corrcoef(s_lead.iloc[:-lag], s_follow.iloc[lag:])[0, 1]\n",
        "                corrs.append(c)\n",
        "\n",
        "        # ê°€ì¥ ë†’ì€ ìƒê´€ê³„ìˆ˜ë¥¼ ì ìˆ˜ë¡œ ì‚¬ìš©\n",
        "        max_corr = max(corrs) if corrs else 0\n",
        "\n",
        "        # LightGBMì˜ ì˜ˆì¸¡ê°’ ê°€ì ¸ì˜¤ê¸°\n",
        "        val = df_lgbm.loc[df_lgbm['pair'] == pair, 'value'].values[0]\n",
        "\n",
        "        scored_candidates.append({\n",
        "            'leading_item_id': lead,\n",
        "            'following_item_id': follow,\n",
        "            'value': val,\n",
        "            'pair': pair,\n",
        "            'score': max_corr\n",
        "        })\n",
        "\n",
        "# 5. ìƒìœ„ 378ê°œ ì„ ì •\n",
        "df_candidates = pd.DataFrame(scored_candidates)\n",
        "df_candidates = df_candidates.sort_values('score', ascending=False) # ì ìˆ˜ ë†’ì€ ìˆœ ì •ë ¬\n",
        "\n",
        "top_378 = df_candidates.head(378)\n",
        "print(f\"ì„ ì •ëœ 378ê°œ ì¤‘ ìµœì†Œ ìƒê´€ê³„ìˆ˜: {top_378['score'].min():.4f}\")\n",
        "\n",
        "# 6. íŒŒì¼ í•©ì¹˜ê¸° (1,620ê°œ + 378ê°œ = 1,998ê°œ)\n",
        "# ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°\n",
        "cols = ['leading_item_id', 'following_item_id', 'value']\n",
        "df_final = pd.concat([df_common[cols], top_378[cols]])\n",
        "\n",
        "# â˜… ì •ìˆ˜ ë°˜ì˜¬ë¦¼ (ê·œì¹™ ì¤€ìˆ˜)\n",
        "df_final['value'] = df_final['value'].round().astype(int)\n",
        "\n",
        "# 7. ì €ì¥\n",
        "output_name = 'submission_gap_closer_1998.csv'\n",
        "df_final.to_csv(output_name, index=False)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"ğŸ‰ íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_name}\")\n",
        "print(f\"ì´ ìŒ ê°œìˆ˜: {len(df_final)}ê°œ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixkAyWC_7QT7",
        "outputId": "f0c0cec5-c0bb-4f89-ecc0-699620338aa4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "íŒŒì¼ ë¡œë“œ ì™„ë£Œ!\n",
            "ë² ì´ìŠ¤ íŒŒì¼ ìŒ: 1620ê°œ\n",
            "ì¶”ê°€ ê²€ì¦í•  í›„ë³´ ìŒ: 538ê°œ\n",
            "ì„ ì •ëœ 378ê°œ ì¤‘ ìµœì†Œ ìƒê´€ê³„ìˆ˜: 0.2938\n",
            "------------------------------\n",
            "ğŸ‰ íŒŒì¼ ìƒì„± ì™„ë£Œ: submission_gap_closer_1998.csv\n",
            "ì´ ìŒ ê°œìˆ˜: 1998ê°œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from scipy.stats import spearmanr\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# ê²½ê³  ë¬´ì‹œ\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ======================================================================================\n",
        "# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "# ======================================================================================\n",
        "print(\"ğŸš€ [Step 1] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...\")\n",
        "try:\n",
        "    train = pd.read_csv('train.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ 'train.csv' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    raise\n",
        "\n",
        "# ë‚ ì§œ ë³€í™˜\n",
        "train['date'] = pd.to_datetime(train['year'].astype(str) + '-' + train['month'].astype(str) + '-01')\n",
        "\n",
        "# í”¼ë²— í…Œì´ë¸” (Index: Date, Columns: Item_ID, Values: Value)\n",
        "pivot_df = train.pivot_table(index='date', columns='item_id', values='value', aggfunc='sum').fillna(0)\n",
        "# ë¡œê·¸ ë³€í™˜ (í•™ìŠµìš©)\n",
        "pivot_log = np.log1p(pivot_df)\n",
        "\n",
        "items = pivot_df.columns.tolist()\n",
        "print(f\"âœ… ì´ ì•„ì´í…œ: {len(items)}ê°œ\")\n",
        "\n",
        "# ======================================================================================\n",
        "# 2. ê³µí–‰ì„± ìŒ ë°œêµ´ (ì—„ê²©í•œ í•„í„°ë§)\n",
        "# ======================================================================================\n",
        "print(\"\\nğŸš€ [Step 2] ê³µí–‰ì„± ìŒ ì ìˆ˜ ì‚°ì¶œ (Smart Cutoff)...\")\n",
        "\n",
        "pair_scores = []\n",
        "# v3 íŒŒì¼ ì°¸ê³ : ì„ê³„ê°’ 0.35 ê·¼ì²˜ê°€ ì •ë‹µë¥ ì´ ë†’ìŒ\n",
        "MIN_THRESHOLD = 0.30  # ìµœì†Œ ì´ ì ìˆ˜ëŠ” ë„˜ì–´ì•¼ í•¨\n",
        "\n",
        "for lead in tqdm(items, desc=\"Scanning Pairs\"):\n",
        "    for follow in items:\n",
        "        if lead == follow: continue\n",
        "\n",
        "        s_lead = pivot_log[lead]\n",
        "        s_follow = pivot_log[follow]\n",
        "\n",
        "        # 1) Pearson (ì„ í˜•)\n",
        "        pearson = s_lead.corr(s_follow)\n",
        "\n",
        "        # 2) Spearman (ë¹„ì„ í˜•)\n",
        "        spearman, _ = spearmanr(s_lead, s_follow)\n",
        "\n",
        "        # 3) TLCC (ì‹œì°¨ 1~6ê°œì›”) - v3 ë¡œì§ ë°˜ì˜\n",
        "        lag_corrs = []\n",
        "        for lag in range(1, 7):\n",
        "            if len(s_lead) > lag:\n",
        "                c = np.corrcoef(s_lead.iloc[:-lag], s_follow.iloc[lag:])[0, 1]\n",
        "                lag_corrs.append(c)\n",
        "        max_lag = max(lag_corrs) if lag_corrs else 0\n",
        "\n",
        "        # ì¢…í•© ì ìˆ˜ (ìµœëŒ“ê°’ ê¸°ì¤€ - í•˜ë‚˜ë¼ë„ í™•ì‹¤í•˜ë©´ ì±„íƒ)\n",
        "        # í‰ê· ë³´ë‹¤ëŠ” Maxê°€ 'ìˆ¨ê²¨ì§„ ê´€ê³„'ë¥¼ ì°¾ëŠ” ë° ìœ ë¦¬í•¨\n",
        "        final_score = max(pearson, spearman, max_lag)\n",
        "\n",
        "        if final_score >= MIN_THRESHOLD:\n",
        "            pair_scores.append({\n",
        "                'pair_id': f\"{lead}_{follow}\",\n",
        "                'lead': lead,\n",
        "                'follow': follow,\n",
        "                'score': final_score\n",
        "            })\n",
        "\n",
        "df_scores = pd.DataFrame(pair_scores).sort_values('score', ascending=False)\n",
        "\n",
        "# ì—¬ê¸°ì„œ ë„ˆë¬´ ë§ì´ ë½‘íˆë©´ ìƒìœ„ 2200ê°œë¡œ ìë¥´ê³ , ì ìœ¼ë©´ ê·¸ëŒ€ë¡œ ê°\n",
        "if len(df_scores) > 2200:\n",
        "    top_pairs = df_scores.head(2200)\n",
        "    print(f\"âš ï¸ ê¸°ì¤€ í†µê³¼ ìŒì´ ë§ì•„ ìƒìœ„ 2,200ê°œë§Œ ì„ íƒí–ˆìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    top_pairs = df_scores\n",
        "    print(f\"âœ… ê¸°ì¤€({MIN_THRESHOLD}) í†µê³¼ ìŒ ëª¨ë‘ ì„ íƒ.\")\n",
        "\n",
        "selected_pairs_set = set(top_pairs['pair_id'])\n",
        "print(f\"ğŸ’ ìµœì¢… ì„ ë°œ: {len(selected_pairs_set)}ê°œ (ìµœì†Œ ì ìˆ˜: {top_pairs['score'].min():.4f})\")\n",
        "\n",
        "# ======================================================================================\n",
        "# 3. ë°ì´í„°ì…‹ êµ¬ì¶• (v3 Feature Engineering ë°˜ì˜)\n",
        "# ======================================================================================\n",
        "print(\"\\nğŸš€ [Step 3] í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶•...\")\n",
        "\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "lags = [1, 2, 3, 6]\n",
        "\n",
        "for _, row in tqdm(top_pairs.iterrows(), total=len(top_pairs), desc=\"Building Dataset\"):\n",
        "    lead = row['lead']\n",
        "    follow = row['follow']\n",
        "    pair_id = row['pair_id']\n",
        "\n",
        "    ts_lead = pivot_log[lead].values\n",
        "    ts_follow = pivot_log[follow].values\n",
        "\n",
        "    # --- Feature ìƒì„± í•¨ìˆ˜ ---\n",
        "    def get_features(t_idx, _lead, _follow):\n",
        "        feat = {}\n",
        "        # Lag Features\n",
        "        for l in lags:\n",
        "            feat[f'lead_lag_{l}'] = _lead[t_idx-l]\n",
        "            feat[f'follow_lag_{l}'] = _follow[t_idx-l]\n",
        "\n",
        "        # Rolling Mean (ì´ë™í‰ê· )\n",
        "        feat['lead_ma_3'] = np.mean(_lead[t_idx-3:t_idx])\n",
        "        feat['follow_ma_3'] = np.mean(_follow[t_idx-3:t_idx])\n",
        "\n",
        "        # Momentum (ë³€í™”ìœ¨) - v3 ì°¸ê³ \n",
        "        if t_idx >= 2:\n",
        "            feat['lead_mom'] = _lead[t_idx-1] - _lead[t_idx-2]\n",
        "            feat['follow_mom'] = _follow[t_idx-1] - _follow[t_idx-2]\n",
        "        else:\n",
        "            feat['lead_mom'] = 0\n",
        "            feat['follow_mom'] = 0\n",
        "\n",
        "        return feat\n",
        "\n",
        "    # 1) í•™ìŠµ ë°ì´í„°\n",
        "    for t in range(max(lags), len(ts_lead)):\n",
        "        sample = {'pair_id': pair_id, 'target': ts_follow[t]}\n",
        "        sample.update(get_features(t, ts_lead, ts_follow))\n",
        "        train_data.append(sample)\n",
        "\n",
        "    # 2) í…ŒìŠ¤íŠ¸ ë°ì´í„° (2025-08)\n",
        "    last_idx = len(ts_lead)\n",
        "    test_sample = {'pair_id': pair_id}\n",
        "    test_sample.update(get_features(last_idx, ts_lead, ts_follow))\n",
        "    test_data.append(test_sample)\n",
        "\n",
        "train_df = pd.DataFrame(train_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "features = [c for c in train_df.columns if c not in ['pair_id', 'target']]\n",
        "X = train_df[features]\n",
        "y = train_df['target']\n",
        "X_test = test_df[features]\n",
        "\n",
        "# ======================================================================================\n",
        "# 4. ëª¨ë¸ë§ (ì•™ìƒë¸”)\n",
        "# ======================================================================================\n",
        "print(\"\\nğŸš€ [Step 4] ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡...\")\n",
        "\n",
        "# LightGBM (ë©”ì¸)\n",
        "lgb_model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.03, num_leaves=31, random_state=42, verbose=-1)\n",
        "lgb_model.fit(X, y)\n",
        "p_lgb = lgb_model.predict(X_test)\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.03, max_depth=6, random_state=42, n_jobs=-1)\n",
        "xgb_model.fit(X, y)\n",
        "p_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestRegressor(n_estimators=300, max_depth=12, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X, y)\n",
        "p_rf = rf_model.predict(X_test)\n",
        "\n",
        "# ì•™ìƒë¸”\n",
        "p_log_avg = (p_lgb + p_xgb + p_rf) / 3\n",
        "final_preds = np.expm1(p_log_avg)\n",
        "\n",
        "# ======================================================================================\n",
        "# 5. í›„ì²˜ë¦¬ ë° ì €ì¥ (Post-processing)\n",
        "# ======================================================================================\n",
        "print(\"\\nğŸš€ [Step 5] í›„ì²˜ë¦¬ ë° ì €ì¥...\")\n",
        "\n",
        "# ê³¼ê±° ìµœëŒ€ê°’ ê¸°ë°˜ Cap ì ìš© (gmu_1.ipynb ì•„ì´ë””ì–´)\n",
        "# ì˜ˆì¸¡ê°’ì´ ê³¼ê±° ìµœëŒ€ê°’ì˜ 1.5ë°°ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì œí•œ (í­ë°œ ë°©ì§€ ì•ˆì „ì¥ì¹˜)\n",
        "final_pred_map = {}\n",
        "\n",
        "for i, pair in enumerate(test_df['pair_id']):\n",
        "    lead, follow = pair.split('_')\n",
        "    pred_val = max(0, final_preds[i])\n",
        "\n",
        "    # Safety Cap\n",
        "    hist_vals = pivot_df[follow].values\n",
        "    max_val = np.max(hist_vals)\n",
        "    if max_val > 0:\n",
        "        # ê³¼ê±° ìµœëŒ€ê°’ë³´ë‹¤ ë„ˆë¬´ í¬ë©´ ì œí•œ (ë³´ìˆ˜ì  ì ‘ê·¼)\n",
        "        cap_val = max_val * 1.5\n",
        "        if pred_val > cap_val:\n",
        "            pred_val = cap_val\n",
        "\n",
        "    final_pred_map[pair] = pred_val\n",
        "\n",
        "# ìµœì¢… íŒŒì¼ ìƒì„±\n",
        "submission_rows = []\n",
        "for lead in items:\n",
        "    for follow in items:\n",
        "        if lead == follow: continue\n",
        "        pair_key = f\"{lead}_{follow}\"\n",
        "\n",
        "        val = 0\n",
        "        if pair_key in final_pred_map:\n",
        "            val = final_pred_map[pair_key]\n",
        "\n",
        "        submission_rows.append({\n",
        "            'leading_item_id': lead,\n",
        "            'following_item_id': follow,\n",
        "            'value': int(round(val))\n",
        "        })\n",
        "\n",
        "final_submission = pd.DataFrame(submission_rows)\n",
        "output_name = 'final_submission_smart_cutoff.csv'\n",
        "final_submission.to_csv(output_name, index=False)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"ğŸ‰ ì™„ë£Œ! íŒŒì¼ëª…: {output_name}\")\n",
        "print(f\"ì˜ˆì¸¡ëœ ìŒ ê°œìˆ˜: {len(final_pred_map)}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqcaZ57wBAN1",
        "outputId": "8bc854a7-4c0e-4789-a1b7-8a21646aad54"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ [Step 1] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...\n",
            "âœ… ì´ ì•„ì´í…œ: 100ê°œ\n",
            "\n",
            "ğŸš€ [Step 2] ê³µí–‰ì„± ìŒ ì ìˆ˜ ì‚°ì¶œ (Smart Cutoff)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scanning Pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:34<00:00,  2.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ ê¸°ì¤€ í†µê³¼ ìŒì´ ë§ì•„ ìƒìœ„ 2,200ê°œë§Œ ì„ íƒí–ˆìŠµë‹ˆë‹¤.\n",
            "ğŸ’ ìµœì¢… ì„ ë°œ: 2200ê°œ (ìµœì†Œ ì ìˆ˜: 0.3141)\n",
            "\n",
            "ğŸš€ [Step 3] í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶•...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2200/2200 [00:01<00:00, 1127.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸš€ [Step 4] ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡...\n",
            "\n",
            "ğŸš€ [Step 5] í›„ì²˜ë¦¬ ë° ì €ì¥...\n",
            "--------------------------------------------------\n",
            "ğŸ‰ ì™„ë£Œ! íŒŒì¼ëª…: final_submission_smart_cutoff.csv\n",
            "ì˜ˆì¸¡ëœ ìŒ ê°œìˆ˜: 2200\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}